from tabnanny import verbose
from NNclasses import *
from POS_tagger import *
import numpy as np
from scipy.special import softmax
import matplotlib.pyplot as plt

act = ActivationLayer(2, 'tanh')

fake_affine_weights = np.random.rand(2, 2)
#print(fake_affine_weights)

X_train = np.array([[1, 1],[1, -1],[-1, 1],[-1, -1]])
y_train = np.array([0, 1, 1, 0])
#print(y_train.size)

milp = MLP([40], ['relu'], 2, 2)
print(milp)
forward = milp.forward_propagation(X_train)
print('forward, ', forward)
back = milp.back_propagation(forward, y_train)
print('backwardL ', back)
milp.update(0.1)

milp.fit(X_train, y_train, 4, 1, 10000)

print(milp.predict(X_train))


train, train_words, train_tags = extract(pathlib.Path('NN-from-scratch/surf.sequoia.train'))
dev, d_words, d_tags = extract(pathlib.Path('NN-from-scratch/surf.sequoia.dev'))
print(dev[0])
print(d_words['de'])

i2w, w2i = vocabulary(train_words, dummy = '<s>')
i2l, l2i = vocabulary(train_tags)

train_X, train_y = prep_examples(train[:50], 2, train_words, w2i, l2i, training=True)
dev_X, dev_y = prep_examples(dev, 2, train_words, w2i, l2i)
#print(train_X[0], train_y[0])

pos_tagger = EmbeddingMLP([36], ['tanh'], len(i2w), 40, 5, len(i2l), verbose = False )
#print(pos_tagger)
print('before training: ', pos_tagger.test(dev_X, dev_y))
''''''
pos_tagger.fit(train_X, train_y, batch_size = 10, learning_rate=0.1, epochs = 100)
print('after_training: ', pos_tagger.test(dev_X, dev_y))
