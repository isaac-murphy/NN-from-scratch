import matplotlib
from NNclasses import *
from POS_tagger import *
import numpy as np
from scipy.special import softmax
import matplotlib
import matplotlib.pyplot as plt


act = ActivationLayer(2, 'sigmoid')

fake_affine_weights = np.random.rand(2, 2)
#print(fake_affine_weights)

X_train = np.array([[1, 1],[1, -1],[-1, 1],[-1, -1]])
y_train = np.array([0, 1, 1, 0])
#print(y_train.size)

milp = MLP([40], ['relu'], 2, 2)
print(milp)
forward = milp.forward_propagation(X_train)
print('forward, ', forward)
back = milp.back_propagation(forward, y_train)
print('backwardL ', back)
milp.update(0.1)

#milp.fit(X_train, y_train, 4, 1, 10000)

print(milp.predict(X_train))


train, train_words, train_tags = extract(pathlib.Path('NN-from-scratch/surf.sequoia.train'))
dev, d_words, d_tags = extract(pathlib.Path('NN-from-scratch/surf.sequoia.dev'))
print(dev[0])
print(d_words['de'])

i2w, w2i = vocabulary(train_words, dummy = '<s>')
i2l, l2i = vocabulary(train_tags)

train_X, train_y = prep_examples(train[:10], 2, train_words, w2i, l2i, training=True)
dev_X, dev_y = prep_examples(dev, 2, train_words, w2i, l2i)
dev_X = np.array(dev_X)
dev_y = np.array(dev_y)
#print(train_X[0], train_y[0])

pos_tagger = EmbeddingMLP([48], ['relu'], len(i2w), 40, 5, len(i2l), verbose = False )
#print(pos_tagger)
print('before training: ', pos_tagger.test(np.array(train_X), np.array(train_y)))
''''''
with Halo(text = 'training', spinner = 'dots'):
    train_s, dev_s = pos_tagger.fit(train_X, train_y, batch_size = 10, learning_rate=0.1, epochs = 60,
             dev_X=dev_X, dev_y = dev_y, patience = 3)
print('after_training: ', pos_tagger.test(np.array(train_X), np.array(train_y)))

print(train_s, dev_s)

plt.plot(train_s, label = 'train')
plt.show()
plt.plot(dev_s, label = 'dev')
plt.legend()
plt.show()
